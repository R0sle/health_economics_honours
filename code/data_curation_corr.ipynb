{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_income = '../split_income_data'\n",
    "filepath_year = '../split_year_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_inc = pd.read_csv(filepath_income + '/test/X_test.csv')\n",
    "test_data_x_inc = test_data_x_inc.drop(columns=['Unnamed: 0'], axis=1)\n",
    "test_data_y_inc = pd.read_csv(filepath_income + '/test/y_test.csv')\n",
    "test_data_y_inc = test_data_y_inc.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "validation_inc = {}\n",
    "for fold in range(0, 5):\n",
    "    vdata_x = pd.read_csv(filepath_income + '/val/X_val_' + str(fold) + '.csv')\n",
    "    vdata_x = vdata_x.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    vdata_y = pd.read_csv(filepath_income + '/val/y_val_' + str(fold) + '.csv')\n",
    "    vdata_y = vdata_y.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    validation_inc[fold] = [vdata_x, vdata_y]\n",
    "\n",
    "train_inc = {}\n",
    "for fold in range(0, 5):\n",
    "    tdata_x1 = pd.read_csv(filepath_income + '/train/X_train_' + str(fold) + '_1.csv')\n",
    "    tdata_x1 = tdata_x1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    tdata_y1 = pd.read_csv(filepath_income + '/train/y_train_' + str(fold) + '_1.csv')\n",
    "    tdata_y1 = tdata_y1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "    train_inc[fold] = [tdata_x1, tdata_y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x_year = pd.read_csv(filepath_year + '/test/X_test.csv')\n",
    "test_data_x_year = test_data_x_year.drop(columns=['Unnamed: 0'], axis=1)\n",
    "test_data_y_year = pd.read_csv(filepath_year + '/test/y_test.csv')\n",
    "test_data_y_year = test_data_y_year.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "validation_year = {}\n",
    "for fold in range(0, 5):\n",
    "    vdata_x = pd.read_csv(filepath_year + '/val/X_val_' + str(fold) + '.csv')\n",
    "    vdata_x = vdata_x.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    vdata_y = pd.read_csv(filepath_year + '/val/y_val_' + str(fold) + '.csv')\n",
    "    vdata_y = vdata_y.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    validation_year[fold] = [vdata_x, vdata_y]\n",
    "\n",
    "train_year = {}\n",
    "for fold in range(0, 5):\n",
    "    tdata_x1 = pd.read_csv(filepath_year + '/train/X_train_' + str(fold) + '_1.csv')\n",
    "    tdata_x1 = tdata_x1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    tdata_y1 = pd.read_csv(filepath_year + '/train/y_train_' + str(fold) + '_1.csv')\n",
    "    tdata_y1 = tdata_y1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "    train_year[fold] = [tdata_x1, tdata_y1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../feature_selection_models/corr_60.pkl', 'rb') as f:\n",
    "    corr_60 = pickle.load(f)\n",
    "with open('../feature_selection_models/corr_70.pkl', 'rb') as f:\n",
    "    corr_70 = pickle.load(f)\n",
    "with open('../feature_selection_models/corr_80.pkl', 'rb') as f:\n",
    "    corr_80 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_col(dataset, threshold):\n",
    "    #excluding columns with greater than the threshold of NAN values \n",
    "    to_drop = []\n",
    "    for col in range(0, dataset.shape[1]):\n",
    "        proportion = (dataset.iloc[:,col].isnull().sum())/(dataset.shape[0])\n",
    "        if proportion > threshold:\n",
    "            to_drop.append(dataset.columns[col])\n",
    "\n",
    "    colthresh = dataset.drop(to_drop, axis=1)\n",
    "\n",
    "    colthresh = colthresh.reset_index(drop=True)\n",
    "\n",
    "    return colthresh\n",
    "\n",
    "def drop_row(dataset, threshold):\n",
    "    \n",
    "    #excluding rows with greater than the threshold of NAN values \n",
    "    to_drop = []\n",
    "    for row in range(1, dataset.shape[0]):\n",
    "        proportion = (dataset.iloc[row, :].isnull().sum())/(dataset.shape[1])\n",
    "        if proportion > threshold:\n",
    "            to_drop.append(row)\n",
    "    \n",
    "    rowthresh = dataset.drop(to_drop, axis=0, inplace=False)\n",
    "    rowthresh = rowthresh.reset_index(drop=True)\n",
    "    \n",
    "    return rowthresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_nan_proportion(dataset):\n",
    "    total_nan = dataset.isnull().sum().sum()\n",
    "    total = dataset.shape[0] * dataset.shape[1]\n",
    "    proportion_total = total_nan/total \n",
    "    return proportion_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_thresholding(dataset_to_threshold, threshold):\n",
    "    last_proportion = total_nan_proportion(dataset_to_threshold)\n",
    "    keep_going = True\n",
    "    dataset_copy = copy.deepcopy(dataset_to_threshold.reset_index())\n",
    "    changes = []\n",
    "\n",
    "    while keep_going == True:\n",
    "\n",
    "        dataset_copy = drop_row(dataset_copy, threshold)\n",
    "        dataset_copy = drop_col(dataset_copy, threshold)\n",
    "        \n",
    "        \n",
    "        current_proportion = total_nan_proportion(dataset_copy)\n",
    "        \n",
    "        #to determine whether dropping the rows and/or columns has caused other rows/columns to breach the threshold\n",
    "        breached = False\n",
    "        for col in range(0, dataset_copy.shape[1]):\n",
    "            proportion = (dataset_copy.iloc[:,col].isnull().sum())/(dataset_copy.shape[0])\n",
    "            if proportion > threshold:\n",
    "                breached = True\n",
    "        for row in range(1, dataset_copy.shape[0]):\n",
    "            proportion = (dataset_copy.iloc[row, :].isnull().sum())/(dataset_copy.shape[1])\n",
    "            if proportion > threshold:\n",
    "                breached = True\n",
    "        \n",
    "        if breached == False:\n",
    "            keep_going = False\n",
    "        else: \n",
    "            changes.append(current_proportion-last_proportion)\n",
    "            last_proportion = current_proportion \n",
    "\n",
    "    return dataset_copy, current_proportion, changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting Correlation Based Subsets of Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inc_60_col = test_data_x_inc.columns.intersection(list(corr_60))\n",
    "test_inc_60 = test_data_x_inc[test_inc_60_col]         \n",
    "\n",
    "test_inc_70_col = test_data_x_inc.columns.intersection(list(corr_70))\n",
    "test_inc_70 = test_data_x_inc[test_inc_70_col]      \n",
    "\n",
    "test_inc_80_col = test_data_x_inc.columns.intersection(list(corr_80))\n",
    "test_inc_80 = test_data_x_inc[test_inc_80_col]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr : {1 : [x, y], 2 : [x, y], etc.}\n",
    "validation_inc_corr = {}\n",
    "\n",
    "v_fold_60_inc = {}\n",
    "v_fold_70_inc = {}\n",
    "v_fold_80_inc = {}\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    val_y = validation_inc[fold][1].copy()\n",
    "    val_x = validation_inc[fold][0].copy()\n",
    "\n",
    "    vx_60_col = val_x.columns.intersection(list(corr_60))\n",
    "    vx_60 = val_x[vx_60_col]  \n",
    "    v_fold_60_inc[fold] = [vx_60, val_y]\n",
    "\n",
    "    vx_70_col = val_x.columns.intersection(list(corr_70))\n",
    "    vx_70 = val_x[vx_70_col]  \n",
    "    v_fold_70_inc[fold] = [vx_70, val_y]\n",
    "\n",
    "    vx_80_col = val_x.columns.intersection(list(corr_80))\n",
    "    vx_80 = val_x[vx_80_col]  \n",
    "    v_fold_80_inc[fold] = [vx_80, val_y]\n",
    "\n",
    "validation_inc_corr['60'] = v_fold_60_inc\n",
    "validation_inc_corr['70'] = v_fold_70_inc\n",
    "validation_inc_corr['80'] = v_fold_80_inc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr : {1 : [x, y], 2 : [x, y], etc.}\n",
    "train_inc_corr = {}\n",
    "\n",
    "t_fold_60_inc = {}\n",
    "t_fold_70_inc = {}\n",
    "t_fold_80_inc = {}\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    t_y = train_inc[fold][1].copy()\n",
    "    t_x = train_inc[fold][0].copy()\n",
    "\n",
    "    tx_60_col = t_x.columns.intersection(list(corr_60))\n",
    "    tx_60 = t_x[tx_60_col]  \n",
    "    t_fold_60_inc[fold] = [tx_60, t_y]\n",
    "\n",
    "    tx_70_col = t_x.columns.intersection(list(corr_70))\n",
    "    tx_70 = t_x[tx_70_col]  \n",
    "    t_fold_70_inc[fold] = [tx_70, t_y]\n",
    "\n",
    "    tx_80_col = t_x.columns.intersection(list(corr_80))\n",
    "    tx_80 = t_x[tx_80_col]  \n",
    "    t_fold_80_inc[fold] = [tx_80, t_y]\n",
    "\n",
    "train_inc_corr['60'] = t_fold_60_inc\n",
    "train_inc_corr['70'] = t_fold_70_inc\n",
    "train_inc_corr['80'] = t_fold_80_inc\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_thresholds = [0.85, 0.90, 0.95, 1]\n",
    "\n",
    "#fold : train_85, rows_left, col_left, train_95, rows_left, col_left, train_100, rows_left, col_left\n",
    "folds_thresh_inc_60 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "folds_thresh_inc_70 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "folds_thresh_inc_80 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "\n",
    "inc_60 = train_inc_corr['60']\n",
    "inc_70 = train_inc_corr['70']\n",
    "inc_80 = train_inc_corr['80']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    data_to_thresholdx_60 = inc_60[fold][0]\n",
    "    data_to_thresholdy_60 = inc_60[fold][1]\n",
    "    data_to_threshold_60 = pd.concat([data_to_thresholdx_60, data_to_thresholdy_60], axis=1)\n",
    "\n",
    "    data_to_thresholdx_70 = inc_70[fold][0]\n",
    "    data_to_thresholdy_70 = inc_70[fold][1]\n",
    "    data_to_threshold_70 = pd.concat([data_to_thresholdx_70, data_to_thresholdy_70], axis=1)\n",
    "\n",
    "    data_to_thresholdx_80 = inc_80[fold][0]\n",
    "    data_to_thresholdy_80 = inc_80[fold][1]\n",
    "    data_to_threshold_80 = pd.concat([data_to_thresholdx_80, data_to_thresholdy_80], axis=1)\n",
    "    \n",
    "    for thresh in needed_thresholds:\n",
    "        new_data60, new_proportion60, new_changes60 = iterative_thresholding(data_to_threshold_60, thresh)\n",
    "        folds_thresh_inc_60[fold].append(new_data60)\n",
    "        folds_thresh_inc_60[fold].append(new_data60.shape[0])\n",
    "        folds_thresh_inc_60[fold].append(new_data60.shape[1])\n",
    "\n",
    "        new_data70, new_proportion70, new_changes70 = iterative_thresholding(data_to_threshold_70, thresh)\n",
    "        folds_thresh_inc_70[fold].append(new_data70)\n",
    "        folds_thresh_inc_70[fold].append(new_data70.shape[0])\n",
    "        folds_thresh_inc_70[fold].append(new_data70.shape[1])\n",
    "\n",
    "        new_data80, new_proportion80, new_changes80 = iterative_thresholding(data_to_threshold_80, thresh)\n",
    "        folds_thresh_inc_80[fold].append(new_data80)\n",
    "        folds_thresh_inc_80[fold].append(new_data80.shape[0])\n",
    "        folds_thresh_inc_80[fold].append(new_data80.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation based sub-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year_60_col = test_data_x_year.columns.intersection(list(corr_60))\n",
    "test_year_60 = test_data_x_year[test_year_60_col]         \n",
    "\n",
    "test_year_70_col = test_data_x_year.columns.intersection(list(corr_70))\n",
    "test_year_70 = test_data_x_year[test_year_70_col]      \n",
    "\n",
    "test_year_80_col = test_data_x_year.columns.intersection(list(corr_80))\n",
    "test_year_80 = test_data_x_year[test_year_80_col]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr : {1 : [x, y], 2 : [x, y], etc.}\n",
    "validation_year_corr = {}\n",
    "\n",
    "v_fold_60_year = {}\n",
    "v_fold_70_year = {}\n",
    "v_fold_80_year = {}\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    val_y = validation_year[fold][1].copy()\n",
    "    val_x = validation_year[fold][0].copy()\n",
    "\n",
    "    vx_60_col = val_x.columns.intersection(list(corr_60))\n",
    "    vx_60 = val_x[vx_60_col]  \n",
    "    v_fold_60_year[fold] = [vx_60, val_y]\n",
    "\n",
    "    vx_70_col = val_x.columns.intersection(list(corr_70))\n",
    "    vx_70 = val_x[vx_70_col]  \n",
    "    v_fold_70_year[fold] = [vx_70, val_y]\n",
    "\n",
    "    vx_80_col = val_x.columns.intersection(list(corr_80))\n",
    "    vx_80 = val_x[vx_80_col]  \n",
    "    v_fold_80_year[fold] = [vx_80, val_y]\n",
    "\n",
    "validation_year_corr['60'] = v_fold_60_year\n",
    "validation_year_corr['70'] = v_fold_70_year\n",
    "validation_year_corr['80'] = v_fold_80_year\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr : {1 : [x, y], 2 : [x, y], etc.}\n",
    "train_year_corr = {}\n",
    "\n",
    "t_fold_60_year = {}\n",
    "t_fold_70_year = {}\n",
    "t_fold_80_year = {}\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    t_y = train_year[fold][1].copy()\n",
    "    t_x = train_year[fold][0].copy()\n",
    "\n",
    "    tx_60_col = t_x.columns.intersection(list(corr_60))\n",
    "    tx_60 = t_x[tx_60_col]  \n",
    "    t_fold_60_year[fold] = [tx_60, t_y]\n",
    "\n",
    "    tx_70_col = t_x.columns.intersection(list(corr_70))\n",
    "    tx_70 = t_x[tx_70_col]  \n",
    "    t_fold_70_year[fold] = [tx_70, t_y]\n",
    "\n",
    "    tx_80_col = t_x.columns.intersection(list(corr_80))\n",
    "    tx_80 = t_x[tx_80_col]  \n",
    "    t_fold_80_year[fold] = [tx_80, t_y]\n",
    "\n",
    "train_year_corr['60'] = t_fold_60_year\n",
    "train_year_corr['70'] = t_fold_70_year\n",
    "train_year_corr['80'] = t_fold_80_year\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_thresholds = [0.85, 0.90, 0.95, 1]\n",
    "\n",
    "#fold : train_85, rows_left, col_left, train_95, rows_left, col_left, train_100, rows_left, col_left\n",
    "folds_thresh_year_60 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "folds_thresh_year_70 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "folds_thresh_year_80 = {0 : [], 1 : [], 2 : [], 3 : [], 4 : []}\n",
    "\n",
    "year_60 = train_year_corr['60']\n",
    "year_70 = train_year_corr['70']\n",
    "year_80 = train_year_corr['80']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    data_to_thresholdx_60 = year_60[fold][0]\n",
    "    data_to_thresholdy_60 = year_60[fold][1]\n",
    "    data_to_threshold_60 = pd.concat([data_to_thresholdx_60, data_to_thresholdy_60], axis=1)\n",
    "\n",
    "    data_to_thresholdx_70 = year_70[fold][0]\n",
    "    data_to_thresholdy_70 = year_70[fold][1]\n",
    "    data_to_threshold_70 = pd.concat([data_to_thresholdx_70, data_to_thresholdy_70], axis=1)\n",
    "\n",
    "    data_to_thresholdx_80 = year_80[fold][0]\n",
    "    data_to_thresholdy_80 = year_80[fold][1]\n",
    "    data_to_threshold_80 = pd.concat([data_to_thresholdx_80, data_to_thresholdy_80], axis=1)\n",
    "    \n",
    "    for thresh in needed_thresholds:\n",
    "        new_data60, new_proportion60, new_changes60 = iterative_thresholding(data_to_threshold_60, thresh)\n",
    "        folds_thresh_year_60[fold].append(new_data60)\n",
    "        folds_thresh_year_60[fold].append(new_data60.shape[0])\n",
    "        folds_thresh_year_60[fold].append(new_data60.shape[1])\n",
    "\n",
    "        new_data70, new_proportion70, new_changes70 = iterative_thresholding(data_to_threshold_70, thresh)\n",
    "        folds_thresh_year_70[fold].append(new_data70)\n",
    "        folds_thresh_year_70[fold].append(new_data70.shape[0])\n",
    "        folds_thresh_year_70[fold].append(new_data70.shape[1])\n",
    "\n",
    "        new_data80, new_proportion80, new_changes80 = iterative_thresholding(data_to_threshold_80, thresh)\n",
    "        folds_thresh_year_80[fold].append(new_data80)\n",
    "        folds_thresh_year_80[fold].append(new_data80.shape[0])\n",
    "        folds_thresh_year_80[fold].append(new_data80.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath_inc = '../fs_corr_income_data/train'\n",
    "val_filepath_inc = '../fs_corr_income_data/val'\n",
    "test_filepath_inc = '../fs_corr_income_data/test'\n",
    "\n",
    "train_filepath_year = '../fs_corr_year_data/train'\n",
    "val_filepath_year = '../fs_corr_year_data/val'\n",
    "test_filepath_year = '../fs_corr_year_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### test year set\n",
    "\n",
    "pd.DataFrame(test_year_60).to_csv(test_filepath_year + '/60/X_test.csv')\n",
    "pd.DataFrame(test_data_y_year).to_csv(test_filepath_year + '/60/y_test.csv')\n",
    "\n",
    "pd.DataFrame(test_year_70).to_csv(test_filepath_year + '/70/X_test.csv')\n",
    "pd.DataFrame(test_data_y_year).to_csv(test_filepath_year + '/70/y_test.csv')\n",
    "\n",
    "pd.DataFrame(test_year_80).to_csv(test_filepath_year + '/80/X_test.csv')\n",
    "pd.DataFrame(test_data_y_year).to_csv(test_filepath_year + '/80/y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### test country set\n",
    "\n",
    "pd.DataFrame(test_inc_60).to_csv(test_filepath_inc + '/60/X_test.csv')\n",
    "pd.DataFrame(test_data_y_inc).to_csv(test_filepath_inc + '/60/y_test.csv')\n",
    "\n",
    "pd.DataFrame(test_inc_70).to_csv(test_filepath_inc + '/70/X_test.csv')\n",
    "pd.DataFrame(test_data_y_inc).to_csv(test_filepath_inc + '/70/y_test.csv')\n",
    "\n",
    "pd.DataFrame(test_inc_80).to_csv(test_filepath_inc + '/80/X_test.csv')\n",
    "pd.DataFrame(test_data_y_inc).to_csv(test_filepath_inc + '/80/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### validation set\n",
    "v_year_60 = validation_year_corr['60']\n",
    "v_year_70 = validation_year_corr['70']\n",
    "v_year_80 = validation_year_corr['80']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    val_input60 = v_year_60[fold][0]\n",
    "    val_labels60 = v_year_60[fold][1]\n",
    "    pd.DataFrame(val_input60).to_csv(val_filepath_year + '/60/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels60).to_csv(val_filepath_year + '/60/y_val_' + str(fold) + '.csv')\n",
    "\n",
    "    val_input70 = v_year_70[fold][0]\n",
    "    val_labels70 = v_year_70[fold][1]\n",
    "    pd.DataFrame(val_input70).to_csv(val_filepath_year + '/70/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels70).to_csv(val_filepath_year + '/70/y_val_' + str(fold) + '.csv')\n",
    "\n",
    "    val_input80 = v_year_80[fold][0]\n",
    "    val_labels80 = v_year_80[fold][1]\n",
    "    pd.DataFrame(val_input80).to_csv(val_filepath_year + '/80/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels80).to_csv(val_filepath_year + '/80/y_val_' + str(fold) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### validation set\n",
    "v_inc_60 = validation_inc_corr['60']\n",
    "v_inc_70 = validation_inc_corr['70']\n",
    "v_inc_80 = validation_inc_corr['80']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    val_input60 = v_inc_60[fold][0]\n",
    "    val_labels60 = v_inc_60[fold][1]\n",
    "    pd.DataFrame(val_input60).to_csv(val_filepath_inc + '/60/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels60).to_csv(val_filepath_inc + '/60/y_val_' + str(fold) + '.csv')\n",
    "\n",
    "    val_input70 = v_inc_70[fold][0]\n",
    "    val_labels70 = v_inc_70[fold][1]\n",
    "    pd.DataFrame(val_input70).to_csv(val_filepath_inc + '/70/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels70).to_csv(val_filepath_inc + '/70/y_val_' + str(fold) + '.csv')\n",
    "\n",
    "    val_input80 = v_inc_80[fold][0]\n",
    "    val_labels80 = v_inc_80[fold][1]\n",
    "    pd.DataFrame(val_input80).to_csv(val_filepath_inc + '/80/X_val_' + str(fold) + '.csv')\n",
    "    pd.DataFrame(val_labels80).to_csv(val_filepath_inc + '/80/y_val_' + str(fold) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### train set\n",
    "t_year_60 = train_year_corr['60']\n",
    "t_year_70 = train_year_corr['70']\n",
    "t_year_80 = train_year_corr['80']\n",
    "\n",
    "threshs = ['85', '90', '95', '1']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    for idx, thresh in enumerate(threshs):\n",
    "        idx_multiple = idx*3\n",
    "\n",
    "        data60 = folds_thresh_year_60[fold][idx_multiple]\n",
    "        train_input60 = data60.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop60 = train_input60.drop(columns=['index'], axis=1)\n",
    "        train_labels60 = data60[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop60).to_csv(train_filepath_year + '/60/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels60).to_csv(train_filepath_year + '/60/y_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "\n",
    "        data70 = folds_thresh_year_70[fold][idx_multiple]\n",
    "        train_input70 = data70.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop70 = train_input70.drop(columns=['index'], axis=1)\n",
    "        train_labels70 = data70[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop70).to_csv(train_filepath_year + '/70/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels70).to_csv(train_filepath_year + '/70/y_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "\n",
    "        data80 = folds_thresh_year_80[fold][idx_multiple]\n",
    "        train_input80 = data80.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop80 = train_input80.drop(columns=['index'], axis=1)\n",
    "        train_labels80 = data80[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop80).to_csv(train_filepath_year + '/80/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels80).to_csv(train_filepath_year + '/80/y_train_' + str(fold) + '_' + thresh + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### train set\n",
    "t_inc_60 = train_inc_corr['60']\n",
    "t_inc_70 = train_inc_corr['70']\n",
    "t_inc_80 = train_inc_corr['80']\n",
    "\n",
    "threshs = ['85', '90', '95', '1']\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    for idx, thresh in enumerate(threshs):\n",
    "        idx_multiple = idx*3\n",
    "\n",
    "        data60 = folds_thresh_inc_60[fold][idx_multiple]\n",
    "        train_input60 = data60.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop60 = train_input60.drop(columns=['index'], axis=1)\n",
    "        train_labels60 = data60[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop60).to_csv(train_filepath_inc + '/60/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels60).to_csv(train_filepath_inc + '/60/y_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "\n",
    "        data70 = folds_thresh_inc_70[fold][idx_multiple]\n",
    "        train_input70 = data70.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop70 = train_input70.drop(columns=['index'], axis=1)\n",
    "        train_labels70 = data70[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop70).to_csv(train_filepath_inc + '/70/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels70).to_csv(train_filepath_inc + '/70/y_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "\n",
    "        data80 = folds_thresh_inc_80[fold][idx_multiple]\n",
    "        train_input80 = data80.drop(['Maternal mortality ratio (national estimate, per 100,000 live births)'], axis=1)\n",
    "        train_input_idx_drop80 = train_input80.drop(columns=['index'], axis=1)\n",
    "        train_labels80 = data80[['Maternal mortality ratio (national estimate, per 100,000 live births)']]\n",
    "        pd.DataFrame(train_input_idx_drop80).to_csv(train_filepath_inc + '/80/X_train_' + str(fold) + '_' + thresh + '.csv')\n",
    "        pd.DataFrame(train_labels80).to_csv(train_filepath_inc + '/80/y_train_' + str(fold) + '_' + thresh + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Literature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
