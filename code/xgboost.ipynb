{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna.visualization as vis\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../split_income_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x = pd.read_csv(filepath + '/test/X_test.csv')\n",
    "test_data_x = test_data_x.drop(columns=['Unnamed: 0'], axis=1)\n",
    "test_data_y = pd.read_csv(filepath + '/test/y_test.csv')\n",
    "test_data_y = test_data_y.drop(columns=['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = {}\n",
    "for fold in range(0, 5):\n",
    "    vdata_x = pd.read_csv(filepath + '/val/X_val_' + str(fold) + '.csv')\n",
    "    vdata_x = vdata_x.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    vdata_y = pd.read_csv(filepath + '/val/y_val_' + str(fold) + '.csv')\n",
    "    vdata_y = vdata_y.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    validation[fold] = [vdata_x, vdata_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "for fold in range(0, 5):\n",
    "    tdata_x85 = pd.read_csv(filepath + '/train/X_train_' + str(fold) + '_85.csv')\n",
    "    tdata_x85 = tdata_x85.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    tdata_y85 = pd.read_csv(filepath + '/train/y_train_' + str(fold) + '_85.csv')\n",
    "    tdata_y85 = tdata_y85.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "    tdata_x95 = pd.read_csv(filepath + '/train/X_train_' + str(fold) + '_95.csv')\n",
    "    tdata_x95 = tdata_x95.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    tdata_y95 = pd.read_csv(filepath + '/train/y_train_' + str(fold) + '_95.csv')\n",
    "    tdata_y95 = tdata_y95.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "    tdata_x1 = pd.read_csv(filepath + '/train/X_train_' + str(fold) + '_1.csv')\n",
    "    tdata_x1 = tdata_x1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    tdata_y1 = pd.read_csv(filepath + '/train/y_train_' + str(fold) + '_1.csv')\n",
    "    tdata_y1 = tdata_y1.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "    train[fold] = [tdata_x85, tdata_y85, tdata_x95, tdata_y95, tdata_x1, tdata_y1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, x_train, y_train, x_val, y_val):\n",
    "\n",
    "    n_trees = trial.suggest_int(\"number_trees\", 10, 300)\n",
    "\n",
    "    max_depth = trial.suggest_int(\"max_tree_depth\", 3, 25)\n",
    "\n",
    "    boosting_type = trial.suggest_categorical(\"boosting_type\", ['gbtree', 'dart'])\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0, 1)\n",
    "    l1_norm = trial.suggest_float(\"l1_norm\", 0, 0.001)\n",
    "    l2_norm = trial.suggest_float(\"l2_norm\", 0, 0.001)\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(enable_categorical=True, missing=np.nan, random_state=42, n_estimators=n_trees, booster=boosting_type, max_depth=max_depth, learning_rate=learning_rate, reg_alpha=l1_norm, reg_lambda=l2_norm, sampling_method=\"uniform\", subsample=subsample)\n",
    "    trained_model = xgb_model.fit(x_train, y_train)\n",
    "    y_pred = trained_model.predict(x_val)\n",
    "    val_loss = mean_squared_error(y_pred, y_val)\n",
    "\n",
    "    return val_loss  # Optuna minimizes this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-20 13:11:11,950] A new study created in memory with name: no-name-bb71a7ce-ce50-44d2-89a4-ea52d993078d\n",
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-20 13:11:54,444] Trial 0 finished with value: 128710.546875 and parameters: {'number_trees': 136, 'max_tree_depth': 13, 'boosting_type': 'dart', 'subsample': 0.4009062069979048, 'learning_rate': 0.6885382959792026, 'l1_norm': 0.0008403441090975851, 'l2_norm': 0.0004571816848596533}. Best is trial 0 with value: 128710.546875.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:11:54] WARNING: /Users/runner/work/xgboost/xgboost/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-20 13:12:09,898] Trial 1 finished with value: 176648.078125 and parameters: {'number_trees': 73, 'max_tree_depth': 27, 'boosting_type': 'dart', 'subsample': 0.5820987155609609, 'learning_rate': 0.9647443668184515, 'l1_norm': 0.0004071480155395352, 'l2_norm': 0.0009061029734098352}. Best is trial 0 with value: 128710.546875.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:12:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"boosting_type\", \"l1_norm\", \"l2_norm\", \"max_tree_depth\", \"number_trees\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "test_data_x['setting'] = test_data_x['setting'].astype(\"category\")\n",
    "\n",
    "val_input_data = validation[0][0]\n",
    "val_input_data['setting'] = val_input_data['setting'].astype(\"category\")\n",
    "\n",
    "train_input_data = train[0][2 * 2]\n",
    "train_input_data['setting'] = train_input_data['setting'].astype(\"category\")\n",
    "\n",
    "#Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, train_input_data, train[0][2 * 2 + 1], val_input_data, validation[0][1]), n_trials=2)\n",
    "best_model = xgb.XGBRegressor(**study.best_params, enable_categorical=True)\n",
    "best_model.fit(train_input_data, train[0][2 * 2 + 1])\n",
    "\n",
    "best_pred = best_model.predict(val_input_data)\n",
    "best_val_loss = mean_squared_error(best_pred, validation[0][1])\n",
    "\n",
    "best_pred_test = best_model.predict(test_data_x)\n",
    "best_test_loss = mean_squared_error(best_pred_test, test_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../split_income_models/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-20 17:37:39,690] A new study created in memory with name: no-name-c45944f8-6b88-4899-9a5a-56e8e6753944\n",
      "[I 2025-07-20 17:37:57,277] Trial 0 finished with value: 155542.421875 and parameters: {'number_trees': 145, 'max_tree_depth': 15, 'boosting_type': 'gbtree', 'subsample': 0.5939903151480914, 'learning_rate': 0.4683125828620116, 'l1_norm': 5.3342299962080286e-05, 'l2_norm': 0.0004645730175161804}. Best is trial 0 with value: 155542.421875.\n",
      "[I 2025-07-20 17:39:00,348] Trial 1 finished with value: 136646.234375 and parameters: {'number_trees': 64, 'max_tree_depth': 20, 'boosting_type': 'dart', 'subsample': 0.9706919131590418, 'learning_rate': 0.287743125241696, 'l1_norm': 6.077657177073425e-06, 'l2_norm': 1.549226729562081e-05}. Best is trial 1 with value: 136646.234375.\n",
      "[I 2025-07-20 17:39:18,402] Trial 2 finished with value: 112908.4921875 and parameters: {'number_trees': 167, 'max_tree_depth': 16, 'boosting_type': 'gbtree', 'subsample': 0.9174445945310687, 'learning_rate': 0.4313817192227999, 'l1_norm': 0.0004206620117229125, 'l2_norm': 0.00022180489182298613}. Best is trial 2 with value: 112908.4921875.\n",
      "[I 2025-07-20 17:40:35,773] Trial 3 finished with value: 126528.9375 and parameters: {'number_trees': 179, 'max_tree_depth': 16, 'boosting_type': 'gbtree', 'subsample': 0.7821839713304185, 'learning_rate': 0.1663461072346385, 'l1_norm': 0.00019264911448028366, 'l2_norm': 0.00032834629626918733}. Best is trial 2 with value: 112908.4921875.\n",
      "[I 2025-07-20 17:49:44,290] Trial 4 finished with value: 96682.6875 and parameters: {'number_trees': 300, 'max_tree_depth': 20, 'boosting_type': 'dart', 'subsample': 0.2553828231990488, 'learning_rate': 0.18209414037266847, 'l1_norm': 0.0005446000610570882, 'l2_norm': 0.00037064878770522137}. Best is trial 4 with value: 96682.6875.\n",
      "[I 2025-07-20 17:49:44,982] Trial 5 finished with value: 84875.453125 and parameters: {'number_trees': 10, 'max_tree_depth': 6, 'boosting_type': 'gbtree', 'subsample': 0.12726226886055747, 'learning_rate': 0.4908936961931989, 'l1_norm': 0.0006133267571576669, 'l2_norm': 2.3172310667450625e-05}. Best is trial 5 with value: 84875.453125.\n",
      "[I 2025-07-20 17:50:14,736] Trial 6 finished with value: 152202.890625 and parameters: {'number_trees': 248, 'max_tree_depth': 22, 'boosting_type': 'gbtree', 'subsample': 0.8777219717852223, 'learning_rate': 0.45627148530108097, 'l1_norm': 0.000315803669270891, 'l2_norm': 0.0002436603632156027}. Best is trial 5 with value: 84875.453125.\n",
      "[I 2025-07-20 17:50:23,125] Trial 7 finished with value: 208653.390625 and parameters: {'number_trees': 28, 'max_tree_depth': 12, 'boosting_type': 'dart', 'subsample': 0.7392557521445183, 'learning_rate': 0.8133112957746945, 'l1_norm': 0.0006149249610121313, 'l2_norm': 0.0006416928252958593}. Best is trial 5 with value: 84875.453125.\n",
      "[I 2025-07-20 17:53:01,794] Trial 8 finished with value: 91125.6015625 and parameters: {'number_trees': 167, 'max_tree_depth': 24, 'boosting_type': 'dart', 'subsample': 0.8503972943450707, 'learning_rate': 0.8587459220198761, 'l1_norm': 0.0009615871584591497, 'l2_norm': 0.00023835880610888717}. Best is trial 5 with value: 84875.453125.\n",
      "[I 2025-07-20 17:53:13,990] Trial 9 finished with value: 73095.515625 and parameters: {'number_trees': 138, 'max_tree_depth': 14, 'boosting_type': 'gbtree', 'subsample': 0.8369093826006299, 'learning_rate': 0.8252109853777713, 'l1_norm': 0.0009992801338664116, 'l2_norm': 6.767315742796865e-05}. Best is trial 9 with value: 73095.515625.\n",
      "[I 2025-07-20 17:53:15,550] Trial 10 finished with value: 156214.953125 and parameters: {'number_trees': 96, 'max_tree_depth': 3, 'boosting_type': 'gbtree', 'subsample': 0.45399711929396497, 'learning_rate': 0.980686632102322, 'l1_norm': 0.0008481722899454118, 'l2_norm': 0.0009614831267844529}. Best is trial 9 with value: 73095.515625.\n",
      "[I 2025-07-20 17:53:16,257] Trial 11 finished with value: 149833.15625 and parameters: {'number_trees': 11, 'max_tree_depth': 7, 'boosting_type': 'gbtree', 'subsample': 0.18890961681805102, 'learning_rate': 0.6559685919425323, 'l1_norm': 0.0007293917029032079, 'l2_norm': 5.179237247751337e-06}. Best is trial 9 with value: 73095.515625.\n",
      "[W 2025-07-20 17:53:26,445] Trial 12 failed with parameters: {'number_trees': 101, 'max_tree_depth': 9, 'boosting_type': 'gbtree', 'subsample': 0.38328330735548144, 'learning_rate': 0.6565627286492689, 'l1_norm': 0.0009900677349148738, 'l2_norm': 0.00011009747368727601} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/jd/0gmk2m5x1cl2hh0v_hsx43q40000gn/T/ipykernel_7619/1005683626.py\", line 15, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, train_input_data, train[fold][thresh_idx * 2 + 1], val_input_data, validation[fold][1]), n_trials=300)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/jd/0gmk2m5x1cl2hh0v_hsx43q40000gn/T/ipykernel_7619/44626517.py\", line 15, in objective\n",
      "    trained_model = xgb_model.fit(x_train, y_train)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-07-20 17:53:26,453] Trial 12 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Create a study object and optimize the objective function.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, train_input_data, train[fold][thresh_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], val_input_data, validation[fold][\u001b[38;5;241m1\u001b[39m]), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     16\u001b[0m best_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params, enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(train_input_data, train[fold][thresh_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[102], line 15\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Create a study object and optimize the objective function.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, train_input_data, train[fold][thresh_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], val_input_data, validation[fold][\u001b[38;5;241m1\u001b[39m]), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m     16\u001b[0m best_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params, enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(train_input_data, train[fold][thresh_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[101], line 15\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m     12\u001b[0m l2_norm \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     14\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, missing\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39mn_trees, booster\u001b[38;5;241m=\u001b[39mboosting_type, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, reg_alpha\u001b[38;5;241m=\u001b[39ml1_norm, reg_lambda\u001b[38;5;241m=\u001b[39ml2_norm, sampling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m, subsample\u001b[38;5;241m=\u001b[39msubsample)\n\u001b[0;32m---> 15\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[1;32m     16\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m trained_model\u001b[38;5;241m.\u001b[39mpredict(x_val)\n\u001b[1;32m     17\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m mean_squared_error(y_pred, y_val)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1247\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1248\u001b[0m     params,\n\u001b[1;32m   1249\u001b[0m     train_dmatrix,\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1251\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[1;32m   1252\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1253\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[1;32m   1254\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[1;32m   1255\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m   1256\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   1257\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1258\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1259\u001b[0m )\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/xgboost/training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m-> 2247\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[1;32m   2248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   2249\u001b[0m         )\n\u001b[1;32m   2250\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data_x['setting'] = test_data_x['setting'].astype(\"category\")\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    val_input_data = validation[fold][0]\n",
    "    val_input_data['setting'] = val_input_data['setting'].astype(\"category\")\n",
    "\n",
    "    for thresh_idx, thresh in enumerate(['85', '95', '1']):\n",
    "        if thresh_idx != 2:\n",
    "            continue\n",
    "        train_input_data = train[fold][thresh_idx * 2]\n",
    "        train_input_data['setting'] = train_input_data['setting'].astype(\"category\")\n",
    "\n",
    "        #Create a study object and optimize the objective function.\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, train_input_data, train[fold][thresh_idx * 2 + 1], val_input_data, validation[fold][1]), n_trials=300)\n",
    "        best_model = xgb.XGBRegressor(**study.best_params, enable_categorical=True)\n",
    "        best_model.fit(train_input_data, train[fold][thresh_idx * 2 + 1])\n",
    "        \n",
    "        #save best model \n",
    "        best_model.save_model(output_dir + '/best_model_' + str(fold) + '_' + thresh +  '.json')\n",
    "        joblib.dump(study.best_params, f\"{output_dir}/best_params_{fold}_{thresh}.pkl\")\n",
    "\n",
    "        # Save study for later visualization\n",
    "        joblib.dump(study, f\"{output_dir}/optuna_study_{fold}_{thresh}.pkl\")\n",
    "\n",
    "        summary = {\n",
    "            \"dataset\": str(fold) + '_' + thresh,\n",
    "            \"fold\" : fold,\n",
    "            \"threshold\": thresh,\n",
    "            \"model\": 'xgboost',\n",
    "            \"best_params\": study.best_params,\n",
    "            \"best_optuna_loss\": study.best_value\n",
    "        }\n",
    "\n",
    "        with open(f\"{output_dir}/results_{fold}_{thresh}.json\", \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../split_income_models/xgboost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Skeletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the xgboost model from the JSON file\n",
    "loaded_model = xgb.XGBRegressor()\n",
    "loaded_model.load_model('../split_income_models/xgboost' + '/best_model_test.json')\n",
    "p = loaded_model.predict(val_input_data)\n",
    "loss = mean_squared_error(p, validation[0][1])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the optuna study\n",
    "loaded_study = joblib.load('../split_income_models/.............')\n",
    "\n",
    "vis.plot_optimization_history(loaded_study)\n",
    "vis.plot_param_importances(loaded_study)\n",
    "vis.plot_slice(loaded_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the json summary file\n",
    "\n",
    "with open(\"../split_income_models/xgboost/results_test.json\", \"r\") as f:\n",
    "    summary_loaded = json.load(f)\n",
    "\n",
    "print(summary_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test values for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missing Data Threshold</th>\n",
       "      <th>Fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Threshold 85%</th>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Threshold 95%</th>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">None</th>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            MAPE  MAE  MSE RMSE   R2\n",
       "Missing Data Threshold Fold                         \n",
       "Threshold 85%          1     NaN  NaN  NaN  NaN  NaN\n",
       "                       2     NaN  NaN  NaN  NaN  NaN\n",
       "                       3     NaN  NaN  NaN  NaN  NaN\n",
       "                       4     NaN  NaN  NaN  NaN  NaN\n",
       "                       5     NaN  NaN  NaN  NaN  NaN\n",
       "Threshold 95%          1     NaN  NaN  NaN  NaN  NaN\n",
       "                       2     NaN  NaN  NaN  NaN  NaN\n",
       "                       3     NaN  NaN  NaN  NaN  NaN\n",
       "                       4     NaN  NaN  NaN  NaN  NaN\n",
       "                       5     NaN  NaN  NaN  NaN  NaN\n",
       "None                   1     NaN  NaN  NaN  NaN  NaN\n",
       "                       2     NaN  NaN  NaN  NaN  NaN\n",
       "                       3     NaN  NaN  NaN  NaN  NaN\n",
       "                       4     NaN  NaN  NaN  NaN  NaN\n",
       "                       5     NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the dataframe to hold test results\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "    [('Threshold 85%', '1'), ('Threshold 85%', '2'), ('Threshold 85%', '3'), ('Threshold 85%', '4'), ('Threshold 85%', '5'),\n",
    "     ('Threshold 95%', '1'), ('Threshold 95%', '2'), ('Threshold 95%', '3'), ('Threshold 95%', '4'), ('Threshold 95%', '5'),\n",
    "     ('None', '1'), ('None', '2'), ('None', '3'), ('None', '4'), ('None', '5')],\n",
    "    names=['Missing Data Threshold', 'Fold']\n",
    ")\n",
    "test_stats = pd.DataFrame(index=index, columns=['MAPE', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "\n",
    "test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_x['setting'] = test_data_x['setting'].astype(\"category\")\n",
    "\n",
    "for fold, fold_num in enumerate(['1', '2', '3', '4', '5']):\n",
    "    for thresh, thresh_name in enumerate(['Threshold 85%', 'Threshold 95%', 'None']):\n",
    "        \n",
    "        if thresh == 0:\n",
    "            name = '85'\n",
    "        elif thresh == 1:\n",
    "            name = '95'\n",
    "        else: name = '1'\n",
    "\n",
    "        best_params = joblib.load(f\"{output_dir}/best_params_{fold}_{name}.pkl\")\n",
    "        train_input_data = train[fold][thresh * 2]\n",
    "        train_input_data['setting'] = train_input_data['setting'].astype(\"category\")\n",
    "\n",
    "        loaded_model = xgb.XGBRegressor(**best_params, enable_categorical=True)\n",
    "        train_load = loaded_model.fit(train_input_data, train[fold][thresh * 2 + 1])\n",
    "        predict = train_load.predict(test_data_x)\n",
    "        mse = mean_squared_error(predict, test_data_y)\n",
    "        mae = mean_absolute_error(predict, test_data_y)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(predict, test_data_y)\n",
    "\n",
    "        #to calculate mape\n",
    "        num_predictions = len(predict)\n",
    "        mape = (1/num_predictions) * np.sum(np.abs(predict - test_data_y) / np.max(np.abs(predict), np.abs(test_data_y)))\n",
    "\n",
    "        test_stats.loc[(thresh_name, fold_num), 'MSE'] = mse\n",
    "        test_stats.loc[(thresh_name, fold_num), 'MAE'] = mae\n",
    "        test_stats.loc[(thresh_name, fold_num), 'RMSE'] = rmse\n",
    "        test_stats.loc[(thresh_name, fold_num), 'R2'] = r2\n",
    "        test_stats.loc[(thresh_name, fold_num), 'MAPE'] = mape\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
